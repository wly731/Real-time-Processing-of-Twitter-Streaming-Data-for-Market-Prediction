{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92a203af",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Kafka Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34743430",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # importing the required modules  \n",
    "# from json import loads  \n",
    "# from kafka import KafkaConsumer  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a9f0aa1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# my_consumer = KafkaConsumer(  \n",
    "#     'testnum',  \n",
    "#      bootstrap_servers = ['localhost : 9092'],  \n",
    "#      auto_offset_reset = 'earliest',  \n",
    "#      enable_auto_commit = True,  \n",
    "#      group_id = 'my-group',  \n",
    "#      api_version=(0,11,5),\n",
    "#      value_deserializer = lambda x : loads(x.decode('utf-8'))  \n",
    "#      )  \n",
    "\n",
    "# for message in my_consumer:  \n",
    "#     message = message.value  \n",
    "# #     collection.insert_one(message)  \n",
    "#     print(message)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b11e7",
   "metadata": {},
   "source": [
    "# Building tweepy stream output into a socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream, StreamingClient, StreamRule\n",
    "\n",
    "import socket\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "import anfis\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# !pip install scikit-fuzzy\n",
    "# !pip install pickle5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f955edd3",
   "metadata": {},
   "source": [
    "## Trial with API v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db4cc3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials (shouldn't be made public)\n",
    "api_key = consumer_key = ''\n",
    "api_key_secret = consumer_secret = api_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = access_secret = ''\n",
    "bearer_token = r''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76735d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gainaing access and connecting to Twitter API using Credentials\n",
    "client = tweepy.Client(bearer_token, api_key, api_secret, access_token, access_token_secret)\n",
    "auth = tweepy.OAuth1UserHandler(api_key, api_secret, access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "623be7ea",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MODEL     = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "hf_token = \"hf_token\"\n",
    "\n",
    "# scipy softmax gets the value between 0-1\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import datetime\n",
    "\n",
    "# from textblob import TextBlob\n",
    "# from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import datetime as dt\n",
    "import string\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config    = AutoConfig.from_pretrained(MODEL)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca04398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# import anfis\n",
    "import membership.mfDerivs\n",
    "import membership.membershipfunction\n",
    "import numpy as np\n",
    "from skfuzzy import gaussmf, gbellmf, sigmf\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from pytz import timezone\n",
    "\n",
    "import math \n",
    "import yfinance as yf\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e89954",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# roBERTa Sentiment generator class\n",
    "class processing_class:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.i = 0\n",
    "    \n",
    "    def process(self, text):\n",
    "#         new_text = []\n",
    "#         for t in text.split(\" \"):\n",
    "#             t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "#             t = 'http' if t.startswith('http') else t\n",
    "#             new_text.append(t)\n",
    "\n",
    "#         new_text = \" \".join(new_text)\n",
    "        new_text = text\n",
    "    \n",
    "        encoded_input = tokenizer(text, return_tensors='pt')\n",
    "        \n",
    "#         print(new_text)\n",
    "        \n",
    "        dic = {'positive': np.nan,\\\n",
    "               'neutral': np.nan,\\\n",
    "               'negative': np.nan,\n",
    "              'text': text}\n",
    "        \n",
    "        try:\n",
    "            output = model(**encoded_input)\n",
    "            \n",
    "            scores = output[0][0].detach().numpy()\n",
    "            scores = softmax(scores)\n",
    "            \n",
    "            ranking = np.argsort(scores)\n",
    "            ranking = ranking[::-1]\n",
    "            \n",
    "            for i in range(scores.shape[0]):\n",
    "                \n",
    "                l = config.id2label[ranking[i]] # error?\n",
    "                s = scores[ranking[i]]\n",
    "                dic[l] = s\n",
    "\n",
    "        except:\n",
    "            \n",
    "            print(f\"Error for: {self.i}\")\n",
    "        \n",
    "#         if self.i%100 == 0:\n",
    "#             print(self.i)\n",
    "\n",
    "#         self.i+=1\n",
    "#         print(dic)\n",
    "        dic = pd.Series([dic['positive'], dic['neutral'], dic['negative'], dic['text']])\n",
    "        return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e343d5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x80\\x03cpandas.core.frame\\nDataFrame\\nq\\x00)\\x81q\\x01}q\\x02(X\\x04\\x00\\x00\\x00_mgrq\\x03cpandas.core.internals.managers\\nBlockManager\\nq\\x04cfunctools\\npartial\\nq\\x05cpandas.core.internals.blocks\\nnew_block\\nq\\x06\\x85q\\x07Rq\\x08(h\\x06)}q\\tX\\x04\\x00\\x00\\x00ndimq\\nK\\x02sNtq\\x0bbcnumpy.core.multiarray\\n_reconstruct\\nq\\x0ccnumpy\\nndarray\\nq\\rK\\x00\\x85q\\x0eC\\x01bq\\x0f\\x87q\\x10Rq\\x11(K\\x01K\\x03K\\x03\\x86q\\x12cnumpy\\ndtype\\nq\\x13X\\x02\\x00\\x00\\x00f4q\\x14\\x89\\x88\\x87q\\x15Rq\\x16(K\\x03X\\x01\\x00\\x00\\x00<q\\x17NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00tq\\x18b\\x89C$w\\x91l?=\\xe7-=\\x8f&\\xbb=\\xec}\\x84=\\x10\\x8e\\x8d>\\xfa\\xd2Q?\\xf4\\xb27<\\x85Z.?\\xa0A\\xb6=q\\x19tq\\x1abcbuiltins\\nslice\\nq\\x1bK\\x00K\\x03K\\x01\\x87q\\x1cRq\\x1d\\x86q\\x1eRq\\x1fh\\x05h\\x06\\x85q Rq!(h\\x06)}q\"h\\nK\\x02sNtq#bh\\x0ch\\rK\\x00\\x85q$h\\x0f\\x87q%Rq&(K\\x01K\\x01K\\x03\\x86q\\'h\\x13X\\x02\\x00\\x00\\x00O8q(\\x89\\x88\\x87q)Rq*(K\\x03X\\x01\\x00\\x00\\x00|q+NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK?tq,b\\x89]q-(X\\x0b\\x00\\x00\\x00great stuffq.X\\t\\x00\\x00\\x00bad stuffq/X\\x08\\x00\\x00\\x00ok stuffq0etq1bh\\x1bK\\x03K\\x04K\\x01\\x87q2Rq3\\x86q4Rq5\\x86q6]q7(cpandas.core.indexes.base\\n_new_Index\\nq8cpandas.core.indexes.numeric\\nInt64Index\\nq9}q:(X\\x04\\x00\\x00\\x00dataq;h\\x0ch\\rK\\x00\\x85q<h\\x0f\\x87q=Rq>(K\\x01K\\x04\\x85q?h\\x13X\\x02\\x00\\x00\\x00i8q@\\x89\\x88\\x87qARqB(K\\x03h\\x17NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00tqCb\\x89C \\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00qDtqEbX\\x04\\x00\\x00\\x00nameqFNu\\x86qGRqHh8cpandas.core.indexes.range\\nRangeIndex\\nqI}qJ(hFNX\\x05\\x00\\x00\\x00startqKK\\x00X\\x04\\x00\\x00\\x00stopqLK\\x03X\\x04\\x00\\x00\\x00stepqMK\\x01u\\x86qNRqOe\\x86qPRqQX\\x04\\x00\\x00\\x00_typqRX\\t\\x00\\x00\\x00dataframeqSX\\t\\x00\\x00\\x00_metadataqT]qUX\\x05\\x00\\x00\\x00attrsqV}qWX\\x06\\x00\\x00\\x00_flagsqX}qYX\\x17\\x00\\x00\\x00allows_duplicate_labelsqZ\\x88sub.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bytes = pickle.dumps(df)\n",
    "df_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23e0b54d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"0\":{\"0\":0.9240946174,\"1\":0.0424568541,\"2\":0.0913821384},\"1\":{\"0\":0.0646933019,\"1\":0.2764744759,\"2\":0.8196254969},\"2\":{\"0\":0.0112120993,\"1\":0.6810687184,\"2\":0.0889923573},\"3\":{\"0\":\"great stuff\",\"1\":\"bad stuff\",\"2\":\"ok stuff\"}}'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6a34f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = datetime.now(timezone('US/Eastern'))\n",
    "t.strftime('%Y-%m-%d %I:%M%p')\n",
    "search = f\"{t.year}-{t.month}-{t.day} {t.hour}:{t.minute}:00\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7178ed64",
   "metadata": {},
   "source": [
    "## CHANGE on_data() to OUTPUT THIS STREAM ONTO SOCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07a17303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from anfis import ANFIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c0b13e0",
   "metadata": {
    "code_folding": [
     74,
     77,
     159,
     279
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule marked to delete: 1600511582559166464 - AAPL\n",
      "rule marked to delete: 1600511586988433410 - TSLA\n",
      "rule marked to delete: 1600511591639834626 - META\n",
      "rule marked to delete: 1600511598321438725 - NVDA\n",
      "rule marked to delete: 1600511602691870721 - AMC\n",
      "rule marked to delete: 1600511606940631040 - BBY\n",
      "rule marked to delete: 1600511614020632576 - GME\n",
      "rule marked to delete: 1600511618676310016 - SOFI\n"
     ]
    }
   ],
   "source": [
    "class MyStream2(tweepy.StreamingClient):    \n",
    "    def __init__(self, portNumber, bearer_token):\n",
    "        \n",
    "        # Call parents' init function to initialize with bearer token\n",
    "        tweepy.StreamingClient.__init__(self, bearer_token)\n",
    "        \n",
    "        self.tickers = ['AAPL', 'TSLA', 'META', 'NVDA', 'AMC', 'BBY', 'GME', 'SOFI']\n",
    "        \n",
    "        # Alpaca\n",
    "        self.HEADERS = {\n",
    "            'APCA-API-KEY-ID': \"APCA-API-KEY\",\n",
    "            'APCA-API-SECRET-KEY': \"APCA-SECRET-KEY\"\n",
    "        }\n",
    "        \n",
    "        self.apiserver_domain = 'paper-api.alpaca.markets'\n",
    "        self.eastern = timezone('US/Eastern')\n",
    "        \n",
    "        # dataframes for each ticker\n",
    "        self.dataframes  = {}\n",
    "        self.stockState = {} # -1, 0, 1\n",
    "        \n",
    "        # time the first we had the first tweet for this ticker\n",
    "        self.first_tweet_occurence = {}\n",
    "        \n",
    "        self.data_X = {}\n",
    "        self.data_Y = {}\n",
    "        self.data_consequents = {}\n",
    "        self.data_mf = {}\n",
    "        self.mfc = {}\n",
    "        self.new_anf = {}\n",
    "        \n",
    "        for t in self.tickers:\n",
    "            # Initialize empty dataframe for each ticker\n",
    "            self.dataframes[t] = pd.DataFrame()\n",
    "            \n",
    "            # Initialize model parameters for all tickers\n",
    "            try:\n",
    "                self.data_X[t] = np.load(f'./model_data/{t}_X.npy')\n",
    "                self.data_Y[t] = np.load(f'./model_data/{t}_Y.npy')\n",
    "                self.data_consequents[t] = np.load(f'./model_data/{t}_consequents.npy')\n",
    "                self.data_mf[t] = np.load(f'./model_data/{t}_mf.npy', allow_pickle=True)\n",
    "                self.data_mf[t] = self.data_mf[t].tolist()\n",
    "                self.mfc[t] = membership.membershipfunction.MemFuncs(self.data_mf[t])\n",
    "                \n",
    "                self.new_anf[t] = anfis.ANFIS(self.data_X[t], self.data_Y[t], self.mfc[t])\n",
    "                self.new_anf[t].consequents = self.data_consequents[t]\n",
    "                \n",
    "                self.first_tweet_occurence[t] = None # initialized when we get the first tweet\n",
    "                \n",
    "                self.stockState[t] = 0\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Some error for {t}: {e}\")\n",
    "            \n",
    "#         self.serverSocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "#         self.serverSocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "\n",
    "#         self.serverSocket.bind((\"127.0.0.1\", portNumber))\n",
    "#         self.serverSocket.listen()\n",
    "        \n",
    "        \n",
    "        # Create processing class (NLP)\n",
    "        self.obj = processing_class()\n",
    "\n",
    "#         while True:\n",
    "#             print(\"Waiting for a client to connect...\")\n",
    "# #             self.clientConnected, self.clientAddress = self.serverSocket.accept()\n",
    "#             print(f\"Accepted a connection request.\")\n",
    "#             break\n",
    "        \n",
    "#         print(\"Client has been connected\")\n",
    "    \n",
    "    def on_connect(self):\n",
    "        print(\"Connected to Twitter\")\n",
    "    \n",
    "    def find_ticker(self, txt):\n",
    "        for ticker in self.tickers:\n",
    "            if ticker in txt:\n",
    "                return ticker\n",
    "        return None\n",
    "    \n",
    "    def calc_feat_vector(self, df, momentum):\n",
    "        \n",
    "        # see what format price_data is\n",
    "        diff = None # price_data (alpaca)\n",
    "        \n",
    "        # simply take the mean() of the positive and neg columns\n",
    "        # print(df)\n",
    "\n",
    "        pos = df[0].mean()\n",
    "        neg = df[2].mean()\n",
    "\n",
    "        return [pos, neg, momentum]\n",
    "\n",
    "    def trade(self, ticker, signal, curr_time):\n",
    "        \n",
    "        # print(\"TRADING I AM\")\n",
    "        if signal < 0:\n",
    "            if self.stockState[ticker] == 1:    \n",
    "                print(f\"Sell {ticker}\")\n",
    "                url =  f'https://{self.apiserver_domain}/v2/positions/{ticker}'\n",
    "                x = requests.delete(url, headers = self.HEADERS)\n",
    "                # print(x.text)\n",
    "                \n",
    "                self.stockState[ticker] = 0\n",
    "                return\n",
    "                \n",
    "            elif self.stockState[ticker] == 0:\n",
    "                print(f\"Short {ticker}\")\n",
    "                #  qty = divide the notional by current price\n",
    "                \n",
    "#                 qty = math.floor(1000/self.get_price(ticker, curr_time))\n",
    "                qty = math.floor(1000/self.get_curr_price(ticker))\n",
    "                \n",
    "                # print(f\"quantity {qty}\")\n",
    "                params = {\"symbol\":ticker, \"qty\": qty, \"side\":\"sell\", \"type\":\"market\",\"time_in_force\":\"day\"}\n",
    "                \n",
    "                url =  f'https://{self.apiserver_domain}/v2/orders'\n",
    "                o = requests.post(url, json=params,headers=self.HEADERS)\n",
    "                # print(o.text)\n",
    "                self.stockState[ticker] = -1\n",
    "                \n",
    "            elif self.stockState[ticker] == -1:\n",
    "                print(\"sell signal but do nothing\")\n",
    "                \n",
    "                return\n",
    "        \n",
    "\n",
    "        else: # signal is positive (buy)\n",
    "            if self.stockState[ticker] == 1:\n",
    "                print(\"buy signal but do nothing\")\n",
    "                return\n",
    "            \n",
    "            elif self.stockState[ticker] == 0:\n",
    "                print(f\"buy {ticker}\")\n",
    "                \n",
    "                params = {\"symbol\":ticker, \n",
    "                          \"notional\": \"1000\",\n",
    "                          \"side\":\"buy\",\n",
    "                          \"type\":\"market\",\n",
    "                          \"time_in_force\":\"day\"}\n",
    "                url =  f'https://{self.apiserver_domain}/v2/orders'\n",
    "                \n",
    "                o = requests.post(url, json = params, headers = self.HEADERS)\n",
    "                # print(o.text)\n",
    "                self.stockState[ticker] = 1\n",
    "                return\n",
    "            \n",
    "                \n",
    "            elif self.stockState[ticker] == -1:\n",
    "                print(f\"cover {ticker}\")\n",
    "                url =  f'https://{self.apiserver_domain}/v2/positions/{ticker}'\n",
    "                x = requests.delete(url, headers = self.HEADERS)\n",
    "                # print(x.text)\n",
    "                \n",
    "                self.stockState[ticker] = 0\n",
    "                return\n",
    "          \n",
    "    def get_price(self, ticker, t):\n",
    "        ticker = yf.Ticker(ticker)\n",
    "        df = ticker.history(period = \"1d\", interval=\"1m\")\n",
    "        \n",
    "        # current time        \n",
    "        # search = f\"{t.year}-{t.month}-{t.day} {t.hour}:{t.minute}:00\"\n",
    "        search = t.strftime('%Y-%m-%d %I:%M') + ':00'\n",
    "        \n",
    "        # current stock price\n",
    "        return df.loc[search][\"Close\"]\n",
    "\n",
    "    def get_curr_price(self, ticker):\n",
    "        ticker = yf.Ticker(ticker)\n",
    "        df = ticker.history(period = \"1d\", interval=\"1m\")\n",
    "    \n",
    "        # current stock price\n",
    "        return df[\"Close\"][-1]\n",
    "            \n",
    "    def get_price_at_two_timstamps(self, ticker, t1, t2):        \n",
    "        # access prices at t1 and t2 (already eastern)\n",
    "        p1, p2 = self.get_price(ticker, t1), self.get_price(ticker, t2)\n",
    "        \n",
    "        return (p1-p2)/p2 \n",
    "    \n",
    "    def on_tweet(self, tweet):\n",
    "        \n",
    "        data = tweet.data\n",
    "#         print(f\"{tweet.id} {tweet.created_at} ({tweet.author_id}): {tweet.text}\")\n",
    "#         print(\"-\"*50)\n",
    "                \n",
    "        try:\n",
    "            curr_time = datetime.now(self.eastern)\n",
    "            \n",
    "            # Calculate sentiment score & append the new data to tickers' dataframe\n",
    "            scores_series = self.obj.process(tweet.text)\n",
    "            check_ticker = self.find_ticker(tweet.text)\n",
    "            \n",
    "            if not check_ticker:\n",
    "                return True\n",
    "            \n",
    "\n",
    "            self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n",
    "                                                                                 ignore_index = True)\n",
    "#             print(self.dataframes[check_ticker])\n",
    "            \n",
    "    \n",
    "            # check the first tweet occurence for this ticker\n",
    "            first_tweet_occurence = self.first_tweet_occurence[check_ticker]\n",
    "            \n",
    "            \n",
    "            if not first_tweet_occurence: \n",
    "                # if none, no tweets added so far for this ticker, add it to dataframe (done above) and exit\n",
    "                self.first_tweet_occurence[check_ticker] = curr_time\n",
    "                # print(f\"Set the first tweet occurence for {check_ticker}\")\n",
    "                return True\n",
    "                \n",
    "                \n",
    "            # but if not empty, and more than 5mins elapsed (300)\n",
    "            elif ((curr_time - first_tweet_occurence).total_seconds() >= 300):\n",
    "                \n",
    "                \n",
    "\n",
    "                # alpaca data for ---- price diff @ curr_time, first_tweet_occurence\n",
    "                p_old = self.get_price(check_ticker,first_tweet_occurence)\n",
    "                p_now = self.get_curr_price(check_ticker)\n",
    "                momentum = (p_now-p_old)/p_old\n",
    "                \n",
    "                # self.get_price_at_two_timstamps(check_ticker, curr_time, first_tweet_occurence)\n",
    "                \n",
    "                # momentum = self.get_price_at_two_timstamps(check_ticker,\n",
    "                #                   datetime(2022, 12, 6, 15, 10, 0, 0), datetime(2022, 12, 6, 15, 5, 0, 0))\n",
    "                \n",
    "                \n",
    "                # Extract 3 features / Average them out\n",
    "                feat_vector = self.calc_feat_vector(self.dataframes[check_ticker], momentum)\n",
    "                \n",
    "                # print(f\"About to make prediction for {check_ticker}\")\n",
    "                # print(f\"using feature vector: {feat_vector} with shape {np.array(feat_vector).shape}\")\n",
    "        \n",
    "                # push them through the model -> buy/sell\n",
    "                signal = anfis.predict(self.new_anf[check_ticker], np.array(feat_vector).reshape(1,3))\n",
    "                # Error on_data: operands could not be broadcast together with shapes (8,) (4,)\n",
    "\n",
    "                # print(f\"Made prdiction {signal}\")\n",
    "                print(f\"Saw tweet: {tweet.text}\")\n",
    "                # based on this, perform the trade \n",
    "                self.trade(check_ticker, signal[0][0], curr_time) #####################################\n",
    "\n",
    "                # print(f\"Made trade\")\n",
    "                \n",
    "                # reset the clock to None again for that ticker\n",
    "                self.first_tweet_occurence[check_ticker] = None\n",
    "                \n",
    "                # reset the pandas dataframe\n",
    "                self.dataframes[check_ticker] = pd.DataFrame()\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "#             self.df = self.df.append(scores_series, ignore_index = True)\n",
    "            \n",
    "            # Send entire dataframe to the connected client\n",
    "#             self.clientConnected.send(bytes(self.df.to_json(), 'UTF-8'))\n",
    "            \n",
    "            # For sending last series only\n",
    "#             self.clientConnected.send(bytes(scores_series.to_json(), 'UTF-8'))\n",
    "            \n",
    "            \n",
    "#             self.clientConnected.send(bytes(json.dumps(data), 'UTF-8')) # works\n",
    "            \n",
    "#             self.clientConnected.sendall(bytes(tweet.encode(), encoding=\"utf-8\"))\n",
    "#             self.clientConnected.sendall(bytes(str(tweet), encoding=\"utf-8\"))\n",
    "#             self.clientSocket.sendall(bytes(tweet, encoding=\"utf-8\"))\n",
    "\n",
    "            return True\n",
    "        \n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "            \n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "################ RULES PART STARTS #####################\n",
    "#  stream.get_rules() and stream.delete_rules()\n",
    "# clean-up pre-existing rules\n",
    "\n",
    "def clean_rules(stream):\n",
    "    rule_ids = []\n",
    "    result = stream.get_rules()\n",
    "    \n",
    "    if not result.data:\n",
    "        return\n",
    "    \n",
    "    for rule in result.data:\n",
    "        print(f\"rule marked to delete: {rule.id} - {rule.value}\")\n",
    "        rule_ids.append(rule.id)\n",
    "\n",
    "    if(len(rule_ids) > 0):\n",
    "        stream.delete_rules(rule_ids)\n",
    "    else:\n",
    "        print(\"no rules to delete\")\n",
    "\n",
    "def add_rules(terms):\n",
    "    for term in search_terms:\n",
    "        stream.add_rules(StreamRule(value = term))\n",
    "\n",
    "# Creating Stream object\n",
    "stream = MyStream2(4329, bearer_token)\n",
    "\n",
    "##### <---- change this if want tweets corresp. to limited no. of tickers\n",
    "search_terms = stream.tickers \n",
    "clean_rules(stream)\n",
    "add_rules(search_terms)\n",
    "\n",
    "obj = processing_class()\n",
    "\n",
    "################ RULES PART ENDS #####################\n",
    "\n",
    "# stream.get_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35fa6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# momentum = stream.get_price_at_two_timstamps('AAPL',\n",
    "#                                   datetime(2022, 12, 6, 15, 10, 0, 0), datetime(2022, 12, 6, 15, 5, 0, 0))\n",
    "# stream.get_price('AAPL', datetime(2022, 12, 6, 15, 5, 0, 0))\n",
    "# momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c85c2c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# stream.get_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c048f19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Twitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n",
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n",
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n",
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n",
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n",
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n",
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n",
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n",
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saw tweet: With All of this #MAIL in Ballots, #Votes showing up after #ElectionDay,  taking 2 weeks to count, #Vote harvesting &amp; alleged FRAUD in U.S #elections, I have ZERO faith in U.S #politics .....\n",
      ".\n",
      "$TSLA #Bitcoin $AAPL #Democrats $SPY #Republicans $NFLX #ETH $AMC #Ethereum $GME $ARKK https://t.co/64aG2HX8mw\n",
      "Short AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n",
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saw tweet: I think these ringers will go to 50 ETH @mecinvestments @aakash_dewan @bitdrax @edu_barriguita @shitoff_tattoo @MuneeYusoh @sanii65518447 @META_SCALPER @ScarlettMoeda @KumarAbhyanand1 @alphadropNFT @unique_pass @nopey0601 @MaryAda24629792 https://t.co/TSgITcj8oZ\n",
      "Short META\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saw tweet: RT @ValueAnalyst1: $TSLA Let’s discuss this on today’s Space: https://t.co/Iw2YqlCN8r\n",
      "Short TSLA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saw tweet: 🍿🔥 #AMC #GME Another Still Here @markets 🔥🍿 https://t.co/vufXLqNoRB\n",
      "Short AMC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n",
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n",
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saw tweet: TELL EM BBY https://t.co/K6dio4mi14\n",
      "Short BBY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ps/2vmrn_5130s62cq8m8m118vr0000gn/T/ipykernel_1849/4219427697.py:203: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.dataframes[check_ticker] = self.dataframes[check_ticker].append(scores_series,\\\n"
     ]
    }
   ],
   "source": [
    "stream.filter(expansions=\"author_id\", tweet_fields=\"created_at\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb4e747b",
   "metadata": {},
   "source": [
    "# Testing - Sockets / Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1af31b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "member_descriptor"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweepy.tweet.Tweet.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bfd31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendData(c_socket):\n",
    "    stream.filter(expansions=\"author_id\", tweet_fields=\"created_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = socket.socket() # Create a socket object\n",
    "host = \"127.0.0.1\"  # Localhost\n",
    "port = 5567\n",
    "s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "\n",
    "s.bind((host, port))        # Bind to the port\n",
    "\n",
    "print(\"Listening on port: %s\" % str(port))\n",
    "s.listen(5)                 # Now wait for client connection.\n",
    "c, addr = s.accept()        # Establish connection with client.\n",
    "print( \"Received request from: \" + str(addr))\n",
    "sendData(c)\n",
    "\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e69e24b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43c3b520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to server\n",
      "sent data\n",
      "received this:\n",
      "Hello Client!\n"
     ]
    }
   ],
   "source": [
    "#----- A simple TCP client program in Python using send() function -----\n",
    "\n",
    "import socket\n",
    "\n",
    " \n",
    "\n",
    "# Create a client socket\n",
    "clientSocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    " \n",
    "\n",
    "# Connect to the server\n",
    "clientSocket.connect((\"127.0.0.1\",8787))\n",
    "\n",
    " \n",
    "print(\"connected to server\")\n",
    "\n",
    "# Send data to server\n",
    "data = \"Hello Server!\"\n",
    "clientSocket.send(data.encode())\n",
    "\n",
    "print(\"sent data\")\n",
    "# Receive data from server\n",
    "dataFromServer = clientSocket.recv(1024)\n",
    "\n",
    "print(\"received this:\")\n",
    "# Print to the console\n",
    "\n",
    "print(dataFromServer.decode())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef2d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d0b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.tweepy.org/en/stable/stream.html#tweepy.Stream.on_data\n",
    "# https://docs.tweepy.org/en/stable/streamrule.html#tweepy.StreamRule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d1623c0",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule marked to delete: 1598658873535119360 - AAPL\n"
     ]
    }
   ],
   "source": [
    "# Bot searches for tweets containing certain keywords\n",
    "class MyStream(tweepy.StreamingClient):\n",
    "\n",
    "    # This function gets called when the stream is working\n",
    "    def on_connect(self):\n",
    "        print(\"Connected\")\n",
    "\n",
    "    # This function gets called when a tweet passes the stream    \n",
    "    def on_tweet(self, tweet):\n",
    "        print(f\"{tweet.id} {tweet.created_at} ({tweet.author_id}): {tweet.text}\")\n",
    "        print(\"-\"*50)\n",
    "#         print(\"-------------------------------------------\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "#         # Displaying tweet in console\n",
    "#         if tweet.referenced_tweets == None:\n",
    "#             print(tweet.text)\n",
    "#             client.like(tweet.id)\n",
    "\n",
    "#             # Delay between tweets\n",
    "#             time.sleep(0.5)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Creating Stream object\n",
    "stream = MyStream(bearer_token = bearer_token)\n",
    "\n",
    "################ RULES PART STARTS #####################\n",
    "#  stream.get_rules() and stream.delete_rules()\n",
    "# clean-up pre-existing rules\n",
    "\n",
    "def clean_rules(stream):\n",
    "    rule_ids = []\n",
    "    result = stream.get_rules()\n",
    "    \n",
    "    if not result.data:\n",
    "        return\n",
    "    \n",
    "    for rule in result.data:\n",
    "        print(f\"rule marked to delete: {rule.id} - {rule.value}\")\n",
    "        rule_ids.append(rule.id)\n",
    "\n",
    "    if(len(rule_ids) > 0):\n",
    "        stream.delete_rules(rule_ids)\n",
    "        stream = MyStream(bearer_token)\n",
    "    else:\n",
    "        print(\"no rules to delete\")\n",
    "\n",
    "def add_rules(terms):\n",
    "    for term in search_terms:\n",
    "        stream.add_rules(StreamRule(value = term))\n",
    "\n",
    "search_terms = [\"AAPL\"]\n",
    "clean_rules(stream)\n",
    "add_rules(search_terms)\n",
    "\n",
    "\n",
    "\n",
    "################ RULES PART ENDS #####################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bd83f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(data=[StreamRule(value='AAPL', tag=None, id='1598659005244653571')], includes={}, errors=[], meta={'sent': '2022-12-02T12:43:06.073Z', 'result_count': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream.get_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f9ae5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stream.filter(expansions=\"author_id\",\n",
    "              tweet_fields=\"created_at\")\n",
    "\n",
    "# stream.filter()\n",
    "# stream.filter(tweet_fields=[\"referenced_tweets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cb82499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7186e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsListener(tweepy.Stream):\n",
    "    def __init__(self, csocket):\n",
    "        self.client_socket = csocket\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            msg = json.loads( data )\n",
    "            print( msg['text'].encode('utf-8') )\n",
    "            self.client_socket.send( msg['text'].encode('utf-8') )\n",
    "            return True\n",
    "        \n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "            \n",
    "        return True\n",
    "    \n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fa20b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendData(c_socket):\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "    twitter_stream = Stream(auth, TweetsListener(c_socket))\n",
    "    twitter_stream.filter(track=['ether'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90a97f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# socket.close(socket.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd1855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "s = socket.socket()         # Create a socket object\n",
    "host = \"127.0.0.1\"          # Get local machine name\n",
    "port = 5566                 # Reserve a port for your service.\n",
    "s.bind((host, port))        # Bind to the port\n",
    "\n",
    "print(\"Listening on port: %s\" % str(port))\n",
    "\n",
    "s.listen(5)                 # Now wait for client connection.\n",
    "c, addr = s.accept()        # Establish connection with client.\n",
    "\n",
    "print( \"Received request from: \" + str( addr ) )\n",
    "\n",
    "sendData( c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c413f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "s = socket.socket() # Create a socket object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"127.0.0.1\"  # Localhost\n",
    "port = 5567\n",
    "s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "\n",
    "s.bind((host, port))        # Bind to the port\n",
    "\n",
    "print(\"Listening on port: %s\" % str(port))\n",
    "s.listen(5)                 # Now wait for client connection.\n",
    "c, addr = s.accept()        # Establish connection with client.\n",
    "print( \"Received request from: \" + str(addr))\n",
    "\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52509f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75587383",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "s.connect((host, port))\n",
    "s.send(magic)\n",
    "data = s.recv(1024)\n",
    "s.close()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcip = host\n",
    "port = 5567\n",
    "# magic = \"\\xFE\"\n",
    "\n",
    "\n",
    "while 1:\n",
    "    # Determine whether the server is up or down\n",
    "    try:\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        s.connect((mcip, port))\n",
    "        s.send(magic)\n",
    "        data = s.recv(1024)\n",
    "        s.close()\n",
    "        print(data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7f9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
